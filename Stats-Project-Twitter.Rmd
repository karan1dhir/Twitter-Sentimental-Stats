---
title: "Stats-Project"
author: "Karan Dhir"
date: "27/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r cars}

library(twitteR)
library(ROAuth)
library(httr)
library(base64enc)
library(dplyr)
library(plyr)
library(dismo)
library(wordcloud)
library(RColorBrewer)
library(lubridate)
library(stringr)
library(plotly)
library(ggplot2)
library(tidytext)
library(leaflet)


# Setting up the credentials inorder to call Twitter API.


TwitterClient <- function(search_string,no_of_tweets){
  
  api_key = as.character("mpJ47SSV27W875Coe1VRbI6hs")
  api_secret = as.character("1ukBRBBESV5WZqkAsA0vqeXeUJzKmOQVvmahgfglkeX5qCXluC")
  access_token = as.character("783402164169256960-cn8FYO2fhZZpxbf10dkdiLbJIcbvA7k")
  access_token_secret = as.character("KJzwcBGQoT9s0oOXsGRIxoMsxuYXzlS7cF5rleNHViLsa")
  
  setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
  
  inital_date <- Sys.Date() - 60
  current_date <- Sys.Date(); 
  MTweets <- searchTwitter(search_string,n = no_of_tweets,since =as.character(inital_date),until=as.character(current_date))
  df <- do.call("rbind",lapply(MTweets,as.data.frame))
  return(list(t_str= MTweets,t_df = df))
}

## Cleaning the tweets 

clean_tweets = function(tweet)
{
  # convert to lower case
  tweet = tolower(tweet)
  # remove rt
  tweet = gsub("rt", "", tweet)
  # remove at
  tweet = gsub("@\\w+", "", tweet)
  # remove punctuation
  tweet = gsub("[[:punct:]]", "", tweet)
  # remove numbers
  tweet = gsub("[[:digit:]]", "", tweet)
  # remove links http
  tweet = gsub("http\\w+", "", tweet)
  # remove tabs
  tweet = gsub("[ |\t]{2,}", "", tweet)
  # remove blank spaces at the beginning
  tweet = gsub("^ ", "", tweet)
  # remove blank spaces at the end
  tweet = gsub(" $", "", tweet)
  # some other cleaning text
  tweet = gsub('https://','',tweet)
  tweet = gsub('http://','',tweet)
  tweet = gsub('[^[:graph:]]', ' ',tweet)
  tweet = gsub('[[:punct:]]', '', tweet)
  tweet = gsub('[[:cntrl:]]', '', tweet)
  tweet = gsub('\\d+', '', tweet)
  return(tweet)
}
# Combining the information extracted from the user and the tweets.
combine_tweet_and_userInfo <- function(tweet_obj){
 
  user_info <- lookupUsers(tweet_obj$t_df$screenName)
  user_df <- twListToDF(user_info)
  
  tweet_df <- tweet_obj$t_df %>% 
    full_join(user_df,by = c('screenName')) %>% dplyr::select(-c(longitude,latitude)) %>%
    dplyr::select(text,favorited,favoriteCount,created.x,statusSource,retweetCount,retweetCount,description,name,location)
  
  return (tweet_df)
   
} 
# Get Number of tweets per hour.
tweets_per_hour <- function(tweet_df){

  tweet_df <- tweet_df %>% 
    mutate(
      created = created.x %>% 
        str_remove_all(pattern = '\\+0000') %>%
        parse_date_time(orders = '%y-%m-%d %H%M%S')
    )
  
  tweet_df <- tweet_df %>% 
    mutate(Created_At_Round = created.x %>% round(units = 'hours') %>% as.POSIXct())
  
  plot_tph <- tweet_df %>% 
    dplyr::count(Created_At_Round) %>% 
    ggplot(mapping = aes(x = Created_At_Round, y = n)) +
    theme_light() +
    geom_line() +
    xlab(label = 'Date') +
    ylab(label = NULL) +
    ggtitle(label = 'Number of Tweets per Hour')
  
  plot_tph %>% ggplotly()
}

# create tag cloud corresponding to the most frequent tweets.
most_frequent_Cloud <- function(tweet_df){
  
tokens <- data_frame(text = tweet_df$text) %>%
    unnest_tokens(word, text) %>%
    anti_join(stop_words) %>%
    dplyr::count(word, sort = TRUE)

hashtags <- data.frame(word = tokens$word,freq= tokens$n)
hashtags <- hashtags[which(hashtags$freq > 30),] 
wordcloud(words = hashtags$word,freq=hashtags$freq,min.freq=1,max.words=100,random.order=FALSE,rot.per=0.35,colors=brewer.pal(8,"Dark2"))

}

## API call-out to get all the tweets belonging to a particular hash-tag.
searchString <- "#globalwarming"
no_of_tweets <- 3500
tweet_obj <- TwitterClient(searchString,no_of_tweets)

## Get the user information of the tweets.
tweet_df <- combine_tweet_and_userInfo(tweet_obj)


## Clean all the tweets by removing all the punctuation, stop words, links http, tabs, blank spaces.
filter_tweets <- clean_tweets(tweet_df$text)  


tweet_df <- tweet_df %>% 
  dplyr::select(-c(text))%>% 
  mutate(text = filter_tweets)

## Plot number of tweets per day.
tweets_per_hour(tweet_df)

## Plot tag-Cloud corresponding to words having highest frequency.
most_frequent_Cloud(tweet_df)


#security_info_geo.sf <- st_as_sf(security_info_geo , coords = c("lng", "lat"), crs = "+proj=longlat +datum=WGS84 +ellps=WGS84")

location_new  <- tweet_df %>% 
  filter(is.na(location) == FALSE & location != "") %>% 
  dplyr::count(location, sort = TRUE) %>% 
  slice(1:20)







```

